## configuration file
### do not use quote here, see the annotation for the options.

[Paths]
dir_env = ../data
dir_asv= ../data
dir_output= ../output
map_file=map.tsv


## Model setting for all models
[model_setting]
predictor= env+asv      
## or (env+asv); use (asv) for RNN and LSTM

models= Ridge
# option: Dummy,LinearRegression,Ridge,PLS,PCR,RandomForest,MLP,DNN, RNN,LSTM

## int, < sample size

env_num=all      
## all or int, the first n environmental variables as predictors.

time_steps=3       
## int,steps of previous time points as predictor

for_periods=1
# steps of latter time points as y, it matters for RNN, LSTM

##############################################################
## Model setting for different models
[Ridge]
## ref: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge
alpha=1
# alpha{float, ndarray of shape (n_targets,)}, default=1.0
# Constant that multiplies the L2 term, controlling regularization strength. alpha must be a non-negative float i.e. in [0, inf). default=1

[PLS]
## ref: https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression
n_components=2
# n_componentsint, default=2
# Number of components to keep. Should be in [1, min(n_samples, n_features, n_targets)]

[PCR]
## ref: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA
n_components=None
# n_componentsint, float or ‘mle’, default=None
# Number of components to keep. if n_components is not set all components are kept:
# n_components == min(n_samples, n_features)

[RandomForest]
## ref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn-ensemble-randomforestregressor
n_estimators=100
# n_estimatorsint, default=100
# The number of trees in the forest.

max_features=0.3
# max_features{“sqrt”, “log2”, None}, int or float, default=1.0
# The number of features to consider when looking for the best split:
# The default of 1.0 is equivalent to bagged trees and more randomness can be achieved by setting smaller values, e.g. 0.3.

[MLP]
## https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor
hidden_layer_sizes=100
# hidden_layer_sizes,array-like of shape(n_layers - 2,), default=(100,)
# The ith element represents the number of neurons in the ith hidden layer.

alpha = 0.0001
# alpha,float, default=0.0001
# Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.

learning_rate_init = 0.001
# learning_rate_init,float, default=0.001
# The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.

max_iterint=10000
# max_iter,int, default=200
# Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.

[DNN]
## ref https://keras.io/api/layers/core_layers/dense/ use Dense layer to build a simple DNN
n_layer=2
# n_layer, int. should be >=1. no default

n_unit=32
# n_unit, int, should be >=1, normaly should be 2^n, no default

dropout=0.2
# Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0. 
# For each training example, dropout randomly selects a subset of neurons in the network and temporarily removes them.
[RNN]
# https://keras.io/api/layers/recurrent_layers/simple_rnn/
n_layer=2
# n_layer, int. should be >=1. no default

n_unit=32
# n_unit, int, should be >=1, normaly should be 2^n, no default

dropout=0.2
# Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0. 
# For each training example, dropout randomly selects a subset of neurons in the network and temporarily removes them.
[LSTM]
## ref: https://keras.io/api/layers/recurrent_layers/lstm/
n_layer=2
# n_layer, int. should be >=1. no default

n_unit=32
# n_unit, int, should be >=1, normaly should be 2^n, no default

dropout=0.2
# Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0. 
# For each training example, dropout randomly selects a subset of neurons in the network and temporarily removes them.

