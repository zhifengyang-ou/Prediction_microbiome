## configuration file
### do not use quote here, see the annotation for the options.

[Paths]
dir_env = ../data
dir_asv= ../data
dir_output= ../output
map_file=map.tsv



[model_setting]
predictor=env+asv
## or (env+asv); use (asv) for RNN and LSTM, although env+asv works for them, actually the models use RNN

models=Dummy,GRU,Transformer
# option: Dummy,LinearRegression,Ridge,PLS,PCR,RandomForest,GradientBoostingRegressor, SVR, MLP,DNN, RNN,LSTM,GRU,Transformer,ARIMA,VAR

## int, < sample size

env_num=all      
## all or int, the first n environmental variables as predictors.

time_steps=3
## int,steps of previous time points as predictor

for_periods=1
# steps of latter time points as y, it matters for RNN, LSTM

##############################################################
## Model setting for different models

[GradientBoostingRegressor]
n_estimators = 200        
# Suggested range: 100-1000 This is the number of boosting stages (trees) to be run. Increasing the number of estimators can improve accuracy but will increase computation time.
learning_rate = 0.05      
# Suggested range: 0.01-0.3 Controls how much each tree contributes to the final prediction. A lower value requires more trees to improve performance.
max_depth = 4             
# Suggested range: 3-10 The maximum depth of the individual trees. Deeper trees capture more complex relationships but can also lead to overfitting.
subsample = 0.8           
# Suggested range: 0.5-1.0 Fraction of samples to be used for fitting each base learner. Subsample < 1.0 leads to stochastic gradient boosting, which can help with overfitting.

[SVR]
C = 10000.0                   
# Suggested range: 0.1-not sure Regularization parameter. A higher value makes the model more focused on fitting the training data closely, which may cause overfitting.
epsilon = 0.01             
# Suggested range: 0.01-0.5 Specifies a margin of tolerance where no penalty is given for errors. Higher values allow the model to be less sensitive to noise in the data.
kernel = rbf           
# linear, poly, rbf The kernel choice is critical as it defines the type of decision boundary. 'rbf' is usually a good default, but others like 'linear' or 'poly' may work better in some cases.

[Ridge]
alpha = 1.0               
# Suggested range: 0.01-10

[RandomForest]
n_estimators = 100        
# Suggested range: 50-500
max_features = 0.5        
# Suggested range: 0.1-1.0

[MLP]
hidden_layer_sizes = 100,100   
# Suggested to try deeper networks with more layers
alpha = 0.001              
# Suggested range: 0.0001-0.1
learning_rate_init = 0.001  
# Suggested range: 0.0001-0.01
max_iter = 500             
# Number of training iterations

[DNN]
n_layer = 4                
# Suggested range: 2-10
n_unit = 128               
# Suggested range: 64-512
dropout = 0.2              
# Suggested range: 0.0-0.5
learning_rate = 0.001      
# Suggested range: 0.0001-0.01
epoch = 100                
# Suggested range: 50-500

[RNN]
n_layer = 2                
# Suggested range: 1-4
n_unit = 128               
# Suggested range: 64-512
dropout = 0.2              
# Suggested range: 0.0-0.5
learning_rate = 0.001      
# Suggested range: 0.0001-0.01
epoch = 100                
# Suggested range: 50-500

[LSTM]
n_layer = 2                
# Suggested range: 1-4
n_unit = 128               
# Suggested range: 64-512
dropout = 0.2              
# Suggested range: 0.0-0.5
learning_rate = 0.001      
# Suggested range: 0.0001-0.01
epoch = 100                
# Suggested range: 50-500

[GRU]
n_layer = 2
n_unit = 64
dropout = 0.2
learning_rate = 0.001
epoch = 100

[Transformer]
n_heads = 4
n_units=64
n_layers = 4
dropout = 0.2
learning_rate = 0.1
epoch = 100



[PLS]
n_components = 5            
# Suggested range: 2-10. Higher values increase risk of overfitting.

[PCR]
n_components = 5            
# Suggested range: 2-10. Higher values capture more variance but may overfit.

[ARIMA]
d = 1
# Differentiation time

q = 1
# moving average of error

## NOT USABLE NOW
[pytorch_DNN]
n_layer=2

n_unit=32

dropout=0.2

epoch=1000

learning_rate=0.001

[pytorch_RNN]
n_layer=2

n_unit=32

dropout=0.2

epoch=1000

learning_rate=0.001

[pytorch_LSTM]
n_layer=2

n_unit=32

dropout=0.2

epoch=1000

learning_rate=0.001
